---
title: "Generic ML Demonstration"
author: "Max Welz, Andreas Alfons, Mert Demirer"
date: "09/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a demonstration of an R implementation of Generic ML, as proposed by [Chernozhukov, Demirer, Duflo, & Fern√°ndez-Val (2021)](https://arxiv.org/abs/1712.04802). Note that this implementation is **not** yet an R package, although we intend to create a package for the CRAN based on it.

## Setup

### Install dependencies

```{r}
rm(list = ls()) ; cat("\014")

# Install the required packages if they are not already installed
required.packages <- c("ggplot2", "mlr3", "mlr3learners", 
                       "mvtnorm", "glmnet", "ranger", "e1071",
                       "kknn", "MASS", "nnet", "xgboost", "sandwich", "lmtest")
new.packages      <- required.packages[!(required.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
rm(required.packages, new.packages)
```

### Load functions

```{r}
setwd(dirname(getwd()))
source(paste0(getwd(), "/functions/loader.R"))
```

## Data generation

We generate $n=5,000$ samples that adhere to a simple linear data generating process. We emulate a randomized experiment. There is no treatment effect heterogeneity since the treatment effect is constant at value two. Hence, Generic ML should not indicate the existence of treatment effect heterogeneity.

```{r}
set.seed(1)
num.obs  <- 5000
num.vars <- 5

# ATE parameter
ATE <- 2

# random treatment assignment 
D <- rbinom(num.obs, 1, 0.5) 

# covariates
Z <- mvtnorm::rmvnorm(num.obs, mean = rep(0, num.vars), sigma = diag(num.vars))
colnames(Z) <- paste0("z", 1:num.vars)

# counterfactual outcomes
Y0 <- as.numeric(Z %*% c(2, -3, 0, 1, 2)) + rnorm(num.obs)
Y1 <- ATE + Y0

# observed outcome
Y  <- ifelse(D == 1, Y1, Y0) 
```

## Applying Generic ML

### The core function, `GenericML()`

The core of this implementation is the function `GenericML()`, which is based on the object-riented `mlr3` framework. This function allows the user to flexibly specify machine learners with `mlr3` syntax. We introduce the arguments of `GenericML()` below.

* `Z` A matrix or data frame of the covariates.
* `D` A binary vector of treatment assignment. 
* `Y` The response vector.
* `learner.propensity.score` The machine learner to be used for estimating the propensity scores. Either `'elastic.net'`, `'random.forest'`, or `'tree'`. Can alternatively be specified by using `mlr3` syntax, for example `'mlr3::lrn("ranger", num.trees = 500)'`. See https://mlr3learners.mlr-org.com for a list of `mlr3` learners. Default is `'elastic.net'`.
* `learners.genericML` A vector of strings specifying the machine learners to be used for estimating the BCA and CATE. Either `'elastic.net'`, `'random.forest'`, or `'tree'`. Can alternatively be specified by using `mlr3` syntax, for example `'mlr3::lrn("ranger", num.trees = 500)'`. See https://mlr3learners.mlr-org.com for a list of `mlr3` learners.
* `num.splits` number of sample splits. Default is 100.
* `Z_CLAN` A matrix of variables that shall be considered for the CLAN. If `NULL` (default), then `Z_CLAN = Z`, i.e. CLAN is performed for all variables in `Z`.
* `HT.transformation` logical. If `TRUE`, a HT transformation is applied in BLP and GATES. Default is `FALSE.`
* `X1.variables_BLP` a list controlling the variables that shall be used in the matrix X1 for the BLP regression. The first element of the list, functions_of_Z, needs to be a subset of c("S", "B", "p"), where "p" corresponds to the propensity scores (default is "B"). The seconds element, custom_covariates, is an optional matrix/data frame of custom covariates that shall be included in X1 (default is NULL). The third element, fixed_effects, is a vector of integers, strings, or a factor thereof that indicates group membership of the observations: For each group, a fixed effect will be added (default is NULL). Note that in the final matrix X1, a constant 1 will be silently included so that the regression model has an intercept.
* `X1.variables_GATES` a list controlling the variables that shall be used in the matrix X1 for the GATES regression. The first element of the list, functions_of_Z, needs to be a subset of c("S", "B", "p"), where "p" corresponds to the propensity scores (default is "B"). The seconds element, custom_covariates, is an optional matrix/data frame of custom covariates that shall be included in X1 (default is NULL). The third element, fixed_effects, is a vector of integers, strings, or a factor thereof that indicates group membership of the observations: For each group, a fixed effect will be added (default is NULL). Note that in the final matrix X1, a constant 1 will be silently included if no HT transformation is applied so that the regression model has an intercept.
* `quantile.cutoffs` The cutoff points of quantiles that shall be used for GATES grouping. Default is `c(0.25, 0.5, 0.75)`, which corresponds to the quartiles.
* `differences.control_GATES` a list with two elements called 'group.to.subtract.from' and 'groups.to.be.subtracted'. The first element ('group.to.subtract.from') denotes what shall be the base group to subtract from in GATES; either "most" or "least". The second element ('groups.to.be.subtracted') are the groups to be subtracted from 'group.to.subtract.from', which is a subset of {1,...,K}, where K equals the number of groups.
* `differences.control_CLAN` same as differences.control_GATES, just for CLAN.
* `vcov.control_BLP` a list with two elements called 'estimator' and 'arguments'. The argument 'estimator' is a string specifying the covariance matrix estimator to be used in the BLP regression; specifies a covariance estimator function in the sandwich package (https://cran.r-project.org/web/packages/sandwich/sandwich.pdf). Recommended estimators are "vcovBS", "vcovCL", "vcovHAC", and "vcovHC". Default is 'vcovHC'. The element 'arguments' is a list of arguments that shall be passed to the function specified in the element 'estimator'. Default leads to the (homoskedastic) ordinary least squares covariance matrix estimator. See the reference manual of the sandwich package for details (https://cran.r-project.org/web/packages/sandwich/vignettes/sandwich.pdf).
* `vcov.control_GATES` same as vcov.control_BLP, just for GATES regression
* `equal.group.variances_CLAN` logical. If `TRUE`, the the two within-group variances of the most and least affected group in CLAN are assumed to be equal. Default is `FALSE.`
* `proportion.in.main.set` proportion of samples that shall be in main set. Default is 0.5.
* `significance.level` significance level for VEIN. Default is 0.05.
* `minimum.variation` minimum variation of the predictions before random noise with distribution N(0, var(Y)/20) is added. Default is 1e-05.
* `store.learners` Logical. If `TRUE`, all intermediate results of the learners will be stored. Default is `FALSE.`
* `store.splits` Logical. If `TRUE`, information on the sample splits will be stored. Default is `FALSE`.

### Specifying the arguments of the core function 

```{r}
# quantile cutoffs for the GATES grouping of the estimated CATEs 
quantile.cutoffs         <- c(0.2, 0.4, 0.6, 0.8) # 20%, 40%, 60%, 80% quantiles

# specify the learner of the propensity score (non-penalized logistic regression here). Propensity scores can also directly be supplied.
learner.propensity.score <- "mlr3::lrn('glmnet', lambda = 0, alpha = 1)" 

# specify the considered learners of the BCA and the CATE (here: elastic net, random forest, and SVM)
learners.genericML       <- c("elastic.net", "mlr3::lrn('ranger', num.trees = 100)", "mlr3::lrn('svm')")

# specify the data that shall be used for the CLAN
# here, we use all variables of Z and uniformly distributed random noise
Z_CLAN <- cbind(Z, random = runif(num.obs))

# specify the number of splits 
num.splits               <- 100

# specify if a HT transformation shall be used when estimating BLP and GATES
HT.transformation <- FALSE

# A list controlling the variables that shall be used in the matrix X1 for the BLP and GATES regressions. The first element of the list, functions_of_Z, needs to be a subset of c("S", "B", "p"), where "p" corresponds to the propensity scores (default is "B"). The seconds element, custom_covariates, is an optional matrix/data frame of custom covariates that shall be included in X1 (default is NULL). The third element, fixed_effects, is a vector of integers, strings, or a factor thereof that indicates group membership of the observations: For each group, a fixed effect will be added (default is NULL). Note that in the final matrix X1, a constant 1 will be silently included so that the regression model has an intercept (unless HT transformation is applied in GATES).
X1.variables_BLP    <- list(functions_of_Z = c("B"),
                            custom_covariates = NULL,
                            fixed_effects = NULL)
X1.variables_GATES  <- list(functions_of_Z = c("B"),
                            custom_covariates = NULL,
                            fixed_effects = NULL)

# consider differences between group K (most affected) with groups 1 and 2, respectively.
differences.control_GATES  <- list(group.to.subtract.from = "most",
                                   groups.to.be.subtracted = c(1,2)) 
differences.control_CLAN  <- list(group.to.subtract.from = "most",
                                  groups.to.be.subtracted = c(1,2))

# specify the significance level
significance.level       <- 0.05

# specify minimum variation of predictions before Gaussian noise with variance var(Y)/20 is added.
minimum.variation <- 1e-05

# specify which estimator of the error covariance matrix shall be used in BLP and GATES (standard OLS covariance matrix estimator here)
vcov.control_BLP   <- list(estimator = "vcovHC",
                        arguments = list(type = "const"))
vcov.control_GATES <- list(estimator = "vcovHC",
                        arguments = list(type = "const"))

# specify whether of not it should be assumed that the group variances of the most and least affected groups are equal in CLAN. 
equal.group.variances_CLAN <- FALSE

# specify the proportion of samples that shall be selected in the main set
proportion.in.main.set   <- 0.5

# specify whether or not the splits and auxiliary results of the learners shall be stored
store.splits             <- FALSE
store.learners           <- FALSE

```

### Run the core function on the generated data

We pass the arguments we specified above to `GenericML()`, which returns an instance of the S3 class `GenericML`.

```{r}
# runtime: ~121 seconds with R version 4.1.0 on a Dell Latitude 5300 (i5-8265U CPU @ 1.60GHz √ó 8, 32GB RAM), running on Ubuntu 20.10.
genML <- GenericML(Z = Z, D = D, Y = Y, 
                   learner.propensity.score = learner.propensity.score, 
                   learners.genericML = learners.genericML,
                   num.splits = num.splits,
                   Z_CLAN = Z_CLAN,
                   HT.transformation = HT.transformation, 
                   X1.variables_BLP = X1.variables_BLP, 
                   X1.variables_GATES = X1.variables_GATES,
                   vcov.control_BLP = vcov.control_BLP, 
                   vcov.control_GATES = vcov.control_GATES,
                   quantile.cutoffs = quantile.cutoffs, 
                   differences.control_GATES = differences.control_GATES,
                   differences.control_CLAN = differences.control_CLAN,
                   equal.group.variances_CLAN = equal.group.variances_CLAN,
                   proportion.in.main.set = proportion.in.main.set, 
                   significance.level = significance.level,
                   minimum.variation = minimum.variation,
                   store.splits = store.splits,
                   store.learners = store.learners)
genML
```

## Analyzing the output of `GenericML()`

Aside from some information on its input and the estimated propensity scores, `GenericML()` returns an `GenericML`-object that consists of two main lists:

1. `best.learners`. Contains information on the performance of each learner (as measured by $\hat{\Lambda}$ and $\hat{\bar{\Lambda}}$).
2. `VEIN` for information on the VEIN analysis.

### The returned `best.learners` list

```{r}
# the line below returns the medians of the estimated  \Lambda and \bar{\Lambda}
genML$best.learners$lambda.overview
```

We can see that the SVM is the best learner for estimating the CATEs, as it maximizes the median of $\hat{\Lambda}$:

```{r}
genML$best.learners$best.learner.for.CATE
```

Conversely, the random forest is the best learner for the GATES, as it maximizes the median of $\hat{\bar{\Lambda}}$:

```{r}
genML$best.learners$best.learner.for.GATES
```

### The returned `VEIN` list

This list contains VEIN objects for the BLP (Best Linear Predictor), GATES (Sorted Group Average Treatment Effects), and CLAN (Classification Analysis). Each object adheres to the same structure and contains information on the relevant point estimates, confidence bounds, as well as the sample splitting adjusted $p$-values of two one-tailed $z$-tests (left and right tailed,respectively). Since the significance level was set to 5%, the confidence bounds are at 90% confidence level (due to uncertainty from sample splitting). Moreover, we can specify which learner's VEIN we want to return. We choose the respective best learner here by using the `best.learners` sub-list.

#### VEIN of BLP 

```{r}
round(genML$VEIN$best.learners$BLP, 5)
```

We see that `beta.1` (the estimate of the ATE) is estimated at ~1.99. True ATE is 2, which is contained in the 90% confidence bounds.  Moreover, `beta.2` is clearly not significant (adjusted $p$-values of both one-tailed tests are much larger than 0.05). Hence, there is (correctly) no indication of treatment effect heterogeneity. Moreover, the method `plot.GenericML()` visualizes these results for the BLP:
 
We see that `beta.1` (the estimate of the ATE) is estimated at ~1.99. True ATE is 2, which is contained in the 90% confidence bounds.

Moreover, `beta.2` is clearly not significant (the adjusted $p$-values of both one-tailed tests are much larger than 0.05). Hence, there is (correctly) no indication of treatment effect heterogeneity. Moreover, the method `plot.GenericML()` visualizes these results for the BLP:

```{r, out.width="50%"}
plot.GenericML(genML, type = "BLP", title = "VEIN of BLP") 
```

Note that the method `plot.GenericML()` can visualize the VEIN for each considered learner. The default (which we use here) visualizes the respective best learner.

#### VEIN of GATES

```{r}
round(genML$VEIN$best.learners$GATES, 5)
```

All point estimates for the $\gamma$ coefficients are close to each other. The difference between the most and least affected group is insignificant (adjusted $p$-values of ~1). Hence, there is (correctly) no indication of treatment effect heterogeneity. We again visualize these results with `plot.GenericML()`:

```{r, out.width="50%"}
plot.GenericML(genML, type = "GATES", title = "VEIN of GATES") 
```

Observe that also the estimate of the ATE, along its 90% confidence bounds, are plotted.

#### VEIN of CLAN

CLAN of the variable `z1` (which was a variable in the input object `Z_CLAN`):

```{r}
genML$VEIN$best.learners$CLAN$z1
```

The CLAN correctly indicates that there is no heterogeneity along `z1` (all $p$-values are much larger than 0.05)


```{r, out.width="50%"}
plot.GenericML(genML, type = "CLAN", CLAN.variable = "z1", title = "CLAN of 'z1'") 
```

Visualizing the CLAN for the variable `random`, which was a column in `Z_CLAN`:

```{r, out.width="50%"}
plot.GenericML(genML, type = "CLAN", CLAN.variable = "random", title = "CLAN of 'random'") 
```

Correctly no evidence for heterogeneity along `random`.

## Conclusion

We conclude that in this demonstration, Generic ML correctly did not find evidence for treatment effect heterogeneity.
